# ============================================================
# Jerry's AI-Assisted Disaster
# ============================================================
# Jerry heard about AI and decided it would "fix everything."
# He used an AI chatbot to generate all of his manifests.
# The AI was confidently wrong. Jerry is on a "wellness retreat."
# You have k8sgpt and an LLM. Good luck.
#
# Apply this file, then let k8sgpt tell you what's broken.
# DO NOT read this file before running k8sgpt — let the AI
# tool do the discovery work.
# ============================================================

---
# --- Workload 1: "storefront" ---
# Jerry asked an AI to "make a web server with persistence"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: storefront
  namespace: jerry-ai
  labels:
    app: storefront
    scenario: "1"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: storefront
  template:
    metadata:
      labels:
        app: storefront
    spec:
      containers:
        - name: nginx
          image: nginx:1.27
          ports:
            - containerPort: 80
          volumeMounts:
            - name: data
              mountPath: /usr/share/nginx/html
          resources:
            requests:
              memory: 64Mi
              cpu: 50m
            limits:
              memory: 128Mi
              cpu: 100m
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: storefront-data
---
# Jerry forgot the PVC. The AI didn't tell him he needed one.
# (Deliberately omitted — k8sgpt should catch this)

---
# --- Workload 2: "payment-processor" ---
# Jerry asked an AI to "make a secure payment service"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-processor
  namespace: jerry-ai
  labels:
    app: payment-processor
    scenario: "2"
spec:
  replicas: 2
  selector:
    matchLabels:
      app: payment-processor
  template:
    metadata:
      labels:
        app: payment-processor
    spec:
      containers:
        - name: payment
          image: busybox:1.36
          command: ["/bin/sh", "-c", "echo 'Payment service starting'; echo \"DB=$DB_PASSWORD\"; sleep 3600"]
          env:
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: payment-secrets
                  key: db-password
            - name: API_KEY
              valueFrom:
                secretKeyRef:
                  name: payment-secrets
                  key: stripe-api-key
          resources:
            requests:
              memory: 64Mi
              cpu: 50m
            limits:
              memory: 128Mi
              cpu: 100m
# payment-secrets doesn't exist — k8sgpt should identify both missing keys

---
# --- Workload 3: "recommendation-engine" ---
# Jerry asked an AI to "scale my ML model to handle traffic"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation-engine
  namespace: jerry-ai
  labels:
    app: recommendation-engine
    scenario: "3"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: recommendation-engine
  template:
    metadata:
      labels:
        app: recommendation-engine
    spec:
      containers:
        - name: model
          image: python:3.11-slim
          command: ["/bin/sh", "-c", "pip install flask && python -c \"from flask import Flask; app = Flask(__name__); app.run(host='0.0.0.0', port=8080)\""]
          ports:
            - containerPort: 8080
          resources:
            requests:
              memory: 32Gi
              cpu: 16
            limits:
              memory: 32Gi
              cpu: 16
# The AI recommended "production-grade" resource values.
# 32Gi RAM / 16 CPUs — this pod will never schedule on a kind cluster.

---
# --- Workload 4: "notification-service" ---
# Jerry asked an AI to "send emails from Kubernetes"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: notification-service
  namespace: jerry-ai
  labels:
    app: notification-service
    scenario: "4"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: notification-service
  template:
    metadata:
      labels:
        app: notification-service
    spec:
      containers:
        - name: notifier
          image: busybox:1.36
          command: ["/bin/sh", "-c"]
          args:
            - |
              echo "Loading email config from /etc/notifier/config.yaml"
              cat /etc/notifier/config.yaml
              echo "Starting notification service..."
              sleep 3600
          volumeMounts:
            - name: notifier-config
              mountPath: /etc/notifier
          resources:
            requests:
              memory: 32Mi
              cpu: 25m
            limits:
              memory: 64Mi
              cpu: 50m
      volumes:
        - name: notifier-config
          configMap:
            name: notifier-config
            items:
              - key: config.yaml
                path: config.yaml
---
# The ConfigMap exists but is missing the key the volume mount expects
apiVersion: v1
kind: ConfigMap
metadata:
  name: notifier-config
  namespace: jerry-ai
data:
  # Jerry named it "settings.yaml" — the mount expects "config.yaml"
  settings.yaml: |
    smtp_host: mail.example.com
    smtp_port: 587
    from_address: jerry@example.com

---
# --- Workload 5: "analytics-api" ---
# Jerry asked an AI to "expose my analytics internally"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytics-api
  namespace: jerry-ai
  labels:
    app: analytics
    scenario: "5"
spec:
  replicas: 2
  selector:
    matchLabels:
      app: analytics
  template:
    metadata:
      labels:
        app: analytics
    spec:
      containers:
        - name: analytics
          image: hashicorp/http-echo:latest
          args:
            - "-text=analytics data"
            - "-listen=:9090"
          ports:
            - containerPort: 9090
          resources:
            requests:
              memory: 32Mi
              cpu: 25m
            limits:
              memory: 64Mi
              cpu: 50m
---
# Service targets wrong port — AI got the targetPort wrong
apiVersion: v1
kind: Service
metadata:
  name: analytics-api
  namespace: jerry-ai
  labels:
    app: analytics
    scenario: "5"
spec:
  selector:
    app: analytics
  ports:
    - name: http
      port: 80
      targetPort: 8080
# analytics container listens on 9090, but targetPort is 8080

---
# --- Workload 6: "cache-layer" ---
# Jerry asked an AI to "add Redis caching"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache-layer
  namespace: jerry-ai
  labels:
    app: cache-layer
    scenario: "6"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cache-layer
  template:
    metadata:
      labels:
        app: cache-layer
    spec:
      containers:
        - name: redis
          image: redis:7.2
          ports:
            - containerPort: 6379
          resources:
            requests:
              memory: 64Mi
              cpu: 50m
            limits:
              memory: 128Mi
              cpu: 100m
          readinessProbe:
            exec:
              command:
                - redis-cli
                - -h
                - "postgres.jerry-ai.svc.cluster.local"
                - ping
            initialDelaySeconds: 5
            periodSeconds: 10
# The AI copy-pasted the readinessProbe from a different service.
# It's pinging a postgres host that doesn't exist instead of localhost.
# Redis starts fine but never becomes Ready.
