---
apiVersion: v1
kind: Namespace
metadata:
  name: resource-lab

---
# cpu-burner: two replicas pegging their CPU limit via shell spin-loops.
# kubectl top should show ~450–500m per pod (limit is 500m each).
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-burner
  namespace: resource-lab
  labels:
    tier: compute
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cpu-burner
  template:
    metadata:
      labels:
        app: cpu-burner
        tier: compute
    spec:
      containers:
      - name: burner
        image: busybox:1.36
        command: ["/bin/sh", "-c"]
        args:
        - |
          # Spawn two tight loops so the CPU limit is clearly visible
          while true; do :; done &
          while true; do :; done &
          wait
        resources:
          requests:
            cpu: "250m"
            memory: "64Mi"
          limits:
            cpu: "500m"
            memory: "64Mi"

---
# memory-hog: allocates 250 MiB of Python bytearray and sleeps.
# CPU near 0; memory should read ~270–300 Mi in kubectl top.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: memory-hog
  namespace: resource-lab
  labels:
    tier: storage
spec:
  replicas: 1
  selector:
    matchLabels:
      app: memory-hog
  template:
    metadata:
      labels:
        app: memory-hog
        tier: storage
    spec:
      containers:
      - name: hog
        image: python:3.11-slim
        command: ["python3", "-c"]
        args:
        - |
          buf = bytearray(250 * 1024 * 1024)
          import time; time.sleep(86400)
        resources:
          requests:
            cpu: "50m"
            memory: "300Mi"
          limits:
            cpu: "100m"
            memory: "400Mi"

---
# idle-worker: three nginx replicas with near-zero consumption.
# Demonstrates the gap between requests (reserved) and actual usage.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: idle-worker
  namespace: resource-lab
  labels:
    tier: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: idle-worker
  template:
    metadata:
      labels:
        app: idle-worker
        tier: web
    spec:
      containers:
      - name: web
        image: nginx:1.27
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "10m"
            memory: "32Mi"
          limits:
            cpu: "50m"
            memory: "64Mi"

---
# batch-processor: a Job with moderate CPU that completes in ~60 seconds.
# Use this to observe how completed pods disappear from kubectl top.
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-processor
  namespace: resource-lab
spec:
  completions: 4
  parallelism: 2
  template:
    metadata:
      labels:
        app: batch-processor
        tier: batch
    spec:
      restartPolicy: Never
      containers:
      - name: processor
        image: busybox:1.36
        command: ["/bin/sh", "-c"]
        args:
        - |
          # Moderate CPU work: compress /dev/urandom data repeatedly
          for i in $(seq 1 6); do
            dd if=/dev/urandom bs=1M count=64 2>/dev/null | gzip > /dev/null
          done
          echo "batch done"
        resources:
          requests:
            cpu: "150m"
            memory: "32Mi"
          limits:
            cpu: "300m"
            memory: "64Mi"
